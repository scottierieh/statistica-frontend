
import sys
import json
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import Ridge
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
import matplotlib.pyplot as plt
import seaborn as sns
import io
import base64
import warnings

warnings.filterwarnings('ignore')

def _to_native_type(obj):
    if isinstance(obj, np.integer):
        return int(obj)
    elif isinstance(obj, np.floating):
        if np.isnan(obj) or np.isinf(obj):
            return None
        return float(obj)
    elif isinstance(obj, np.ndarray):
        return obj.tolist()
    return obj

def fig_to_base64(fig):
    buf = io.BytesIO()
    fig.savefig(buf, format='png', bbox_inches='tight')
    plt.close(fig)
    buf.seek(0)
    return f"data:image/png;base64,{base64.b64encode(buf.read()).decode('utf-8')}"

def _generate_interpretation(train_r2, test_r2):
    interpretation = ""
    r2_diff = train_r2 - test_r2
    
    if train_r2 > 0.8 and r2_diff < 0.2:
        interpretation = "The model shows a **Good Fit**. Both training and testing R-squared scores are high and close to each other, indicating that the model generalizes well to new data."
    elif train_r2 > 0.7 and r2_diff > 0.3:
        interpretation = "**Overfitting Warning**. The model performs significantly better on the training data than on the test data. This suggests that the model has learned the training data's noise and may not perform well on unseen data. Consider increasing the alpha value to add more regularization."
    elif train_r2 < 0.5 and test_r2 < 0.5:
        interpretation = "**Underfitting Possible**. Both training and testing R-squared scores are low, suggesting the model is too simple to capture the underlying patterns in the data. The model may not be complex enough."
    else:
        interpretation = "The model's performance is moderate. Review the R-squared values and residuals to assess if the model is sufficient for your needs."
        
    return interpretation.strip()


def main():
    try:
        payload = json.load(sys.stdin)
        data = payload.get('data')
        target = payload.get('target')
        features = payload.get('features')
        alpha = float(payload.get('alpha', 1.0))
        test_size = float(payload.get('test_size', 0.2))

        if not all([data, target, features]):
            raise ValueError("Missing data, target, or features")

        df = pd.DataFrame(data)
        
        X = df[features]
        y = df[target]
        
        # One-hot encode categorical features if any
        X = pd.get_dummies(X, drop_first=True)
        final_features = X.columns.tolist()

        y = pd.to_numeric(y, errors='coerce')
        
        combined = pd.concat([X, y], axis=1).dropna()
        X = combined[final_features]
        y = combined[target]
        
        if X.empty or y.empty:
            raise ValueError("Not enough valid data after cleaning.")

        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)
        
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.transform(X_test)
        
        model = Ridge(alpha=alpha, random_state=42)
        model.fit(X_train_scaled, y_train)
        
        y_pred_test = model.predict(X_test_scaled)
        y_pred_train = model.predict(X_train_scaled)
        
        test_metrics = {
            'r2_score': r2_score(y_test, y_pred_test),
            'rmse': np.sqrt(mean_squared_error(y_test, y_pred_test)),
            'mae': mean_absolute_error(y_test, y_pred_test)
        }
        
        train_metrics = {
            'r2_score': r2_score(y_train, y_pred_train),
            'rmse': np.sqrt(mean_squared_error(y_train, y_pred_train)),
            'mae': mean_absolute_error(y_train, y_pred_train)
        }
        
        interpretation = _generate_interpretation(train_metrics['r2_score'], test_metrics['r2_score'])
        
        results = {
            'metrics': {
                'test': test_metrics,
                'train': train_metrics,
            },
            'coefficients': dict(zip(final_features, model.coef_)),
            'intercept': model.intercept_,
            'alpha': alpha,
            'interpretation': interpretation,
        }
        
        # --- Plotting: Actual vs Predicted (Train vs Test) ---
        fig_main, axes = plt.subplots(2, 1, figsize=(8, 12))
        fig_main.suptitle(f'Ridge Regression Performance (alpha={alpha})', fontsize=16)

        # Train set plot
        axes[0].scatter(y_train, y_pred_train, alpha=0.5, label='(Actual, Predicted)')
        axes[0].plot([y_train.min(), y_train.max()], [y_train.min(), y_train.max()], 'r--', lw=2, label='45° Line (Perfect Fit)')
        axes[0].set_xlabel('Actual Values')
        axes[0].set_ylabel('Predicted Values')
        axes[0].set_title('Train Set Performance')
        axes[0].legend()
        axes[0].grid(True)
        train_text = (
            f"Train R²: {train_metrics['r2_score']:.4f}\n"
            f"Train RMSE: {train_metrics['rmse']:.4f}"
        )
        axes[0].text(0.05, 0.95, train_text, transform=axes[0].transAxes, fontsize=10,
                     verticalalignment='top', bbox=dict(boxstyle='round,pad=0.5', fc='wheat', alpha=0.5))


        # Test set plot
        axes[1].scatter(y_test, y_pred_test, alpha=0.5, label='(Actual, Predicted)')
        axes[1].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2, label='45° Line (Perfect Fit)')
        axes[1].set_xlabel('Actual Values')
        axes[1].set_ylabel('Predicted Values')
        axes[1].set_title('Test Set Performance')
        axes[1].legend()
        axes[1].grid(True)
        test_text = (
            f"Test R²: {test_metrics['r2_score']:.4f}\n"
            f"Test RMSE: {test_metrics['rmse']:.4f}"
        )
        axes[1].text(0.05, 0.95, test_text, transform=axes[1].transAxes, fontsize=10,
                     verticalalignment='top', bbox=dict(boxstyle='round,pad=0.5', fc='wheat', alpha=0.5))

        plt.tight_layout(rect=[0, 0.03, 1, 0.95])
        plot_image = fig_to_base64(fig_main)

        # --- Alpha vs Coefficients Path Plot ---
        alpha_list = np.logspace(-4, 4, 200)
        coefs = []
        for a in alpha_list:
            ridge_iter = Ridge(alpha=a, random_state=42)
            ridge_iter.fit(X_train_scaled, y_train)
            coefs.append(ridge_iter.coef_)
        
        fig_path, ax_path = plt.subplots(figsize=(8, 6))
        ax_path.plot(alpha_list, coefs)
        ax_path.set_xscale('log')
        ax_path.set_xlabel('Alpha (Regularization Strength)')
        ax_path.set_ylabel('Coefficient Value')
        ax_path.set_title('Ridge Coefficients Path')
        ax_path.legend(final_features, bbox_to_anchor=(1.05, 1), loc='upper left', fontsize='small')
        ax_path.grid(True)
        path_plot_image = fig_to_base64(fig_path)

        response = {
            'results': results,
            'plot': plot_image,
            'path_plot': path_plot_image
        }
        
        print(json.dumps(response, default=_to_native_type))

    except Exception as e:
        print(json.dumps({"error": str(e)}), file=sys.stderr)
        sys.exit(1)

if __name__ == '__main__':
    main()
