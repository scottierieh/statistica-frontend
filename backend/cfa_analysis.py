# Pythonì„ í™œìš©í•œ CFA(Confirmatory Factor Analysis) êµ¬í˜„
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.datasets import make_classification, load_iris
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import onnx
import onnxruntime as ort
from skl2onnx import convert_sklearn
from skl2onnx.common.data_types import FloatTensorType
import warnings
warnings.filterwarnings('ignore')

# í•œê¸€ í°íŠ¸ ì„¤ì • (matplotlib)
plt.rcParams['font.family'] = ['DejaVu Sans']
plt.style.use('seaborn-v0_8')

class DataAnalyzer:
    def __init__(self):
        self.data = None
        self.model = None
        self.onnx_model = None
        self.scaler = StandardScaler()
        
    def generate_sample_data(self, n_samples=1000, n_features=4):
        """ìƒ˜í”Œ ë°ì´í„° ìƒì„±"""
        # ë¶„ë¥˜ ë°ì´í„°ì…‹ ìƒì„±
        X, y = make_classification(
            n_samples=n_samples, 
            n_features=n_features,
            n_informative=3,
            n_redundant=1,
            n_clusters_per_class=1,
            random_state=42
        )
        
        # ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë³€í™˜
        feature_names = [f'Feature_{i+1}' for i in range(n_features)]
        self.data = pd.DataFrame(X, columns=feature_names)
        self.data['Target'] = y
        
        print(f"âœ… ìƒ˜í”Œ ë°ì´í„° ìƒì„± ì™„ë£Œ: {n_samples}ê°œ ìƒ˜í”Œ, {n_features}ê°œ íŠ¹ì„±")
        return self.data
    
    def load_iris_data(self):
        """Iris ë°ì´í„°ì…‹ ë¡œë“œ"""
        iris = load_iris()
        self.data = pd.DataFrame(iris.data, columns=iris.feature_names)
        self.data['Target'] = iris.target
        self.data['Species'] = [iris.target_names[i] for i in iris.target]
        
        print("âœ… Iris ë°ì´í„°ì…‹ ë¡œë“œ ì™„ë£Œ")
        return self.data
    
    def explore_data(self):
        """ë°ì´í„° íƒìƒ‰ì  ë¶„ì„"""
        if self.data is None:
            print("âŒ ë°ì´í„°ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return
        
        print("\n=== ë°ì´í„° ê¸°ë³¸ ì •ë³´ ===")
        print(f"ë°ì´í„° í¬ê¸°: {self.data.shape}")
        print(f"ê²°ì¸¡ê°’: {self.data.isnull().sum().sum()}")
        print("\n=== ê¸°ìˆ  í†µê³„ ===")
        print(self.data.describe())
        
        return self.data.describe()
    
    def create_visualizations(self):
        """ë°ì´í„° ì‹œê°í™”"""
        if self.data is None:
            print("âŒ ë°ì´í„°ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return
        
        # Figure ì„¤ì •
        fig = plt.figure(figsize=(15, 12))
        
        # 1. ìƒê´€ê´€ê³„ íˆíŠ¸ë§µ
        plt.subplot(2, 3, 1)
        numeric_cols = self.data.select_dtypes(include=[np.number]).columns
        corr_matrix = self.data[numeric_cols].corr()
        sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0, 
                   square=True, fmt='.2f')
        plt.title('Feature Correlation Heatmap')
        
        # 2. íƒ€ê²Ÿ ë¶„í¬
        plt.subplot(2, 3, 2)
        target_counts = self.data['Target'].value_counts()
        plt.pie(target_counts.values, labels=target_counts.index, autopct='%1.1f%%')
        plt.title('Target Distribution')
        
        # 3. íŠ¹ì„±ë³„ ë¶„í¬ (ì²« ë²ˆì§¸ íŠ¹ì„±)
        plt.subplot(2, 3, 3)
        feature_cols = [col for col in self.data.columns if col not in ['Target', 'Species']]
        if feature_cols:
            self.data[feature_cols[0]].hist(bins=30, alpha=0.7)
            plt.title(f'{feature_cols[0]} Distribution')
            plt.xlabel(feature_cols[0])
            plt.ylabel('Frequency')
        
        # 4. ë°•ìŠ¤í”Œë¡¯ (íƒ€ê²Ÿë³„ ì²« ë²ˆì§¸ íŠ¹ì„±)
        plt.subplot(2, 3, 4)
        if feature_cols:
            sns.boxplot(data=self.data, x='Target', y=feature_cols[0])
            plt.title(f'{feature_cols[0]} by Target')
        
        # 5. ì‚°ì ë„ (ì²« ë‘ íŠ¹ì„±)
        plt.subplot(2, 3, 5)
        if len(feature_cols) >= 2:
            scatter = plt.scatter(self.data[feature_cols[0]], 
                                self.data[feature_cols[1]], 
                                c=self.data['Target'], 
                                alpha=0.6, cmap='viridis')
            plt.xlabel(feature_cols[0])
            plt.ylabel(feature_cols[1])
            plt.title('Feature Scatter Plot')
            plt.colorbar(scatter)
        
        # 6. íŠ¹ì„± ì¤‘ìš”ë„ (ëª¨ë¸ í•™ìŠµ í›„)
        plt.subplot(2, 3, 6)
        if len(feature_cols) > 0:
            # ê°„ë‹¨í•œ ëª¨ë¸ í•™ìŠµ
            X = self.data[feature_cols]
            y = self.data['Target']
            rf = RandomForestClassifier(n_estimators=100, random_state=42)
            rf.fit(X, y)
            
            importance = pd.DataFrame({
                'feature': feature_cols,
                'importance': rf.feature_importances_
            }).sort_values('importance', ascending=True)
            
            plt.barh(importance['feature'], importance['importance'])
            plt.title('Feature Importance')
            plt.xlabel('Importance')
        
        plt.tight_layout()
        plt.show()
        
    def train_model_and_convert_to_onnx(self):
        """ëª¨ë¸ í•™ìŠµ ë° ONNX ë³€í™˜"""
        if self.data is None:
            print("âŒ ë°ì´í„°ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return
        
        # íŠ¹ì„±ê³¼ íƒ€ê²Ÿ ë¶„ë¦¬
        feature_cols = [col for col in self.data.columns if col not in ['Target', 'Species']]
        X = self.data[feature_cols]
        y = self.data['Target']
        
        # ë°ì´í„° ë¶„í• 
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=y
        )
        
        # ë°ì´í„° í‘œì¤€í™”
        X_train_scaled = self.scaler.fit_transform(X_train)
        X_test_scaled = self.scaler.transform(X_test)
        
        # RandomForest ëª¨ë¸ í•™ìŠµ
        self.model = RandomForestClassifier(
            n_estimators=100, 
            max_depth=10, 
            random_state=42
        )
        
        self.model.fit(X_train_scaled, y_train)
        
        # ëª¨ë¸ ì„±ëŠ¥ í‰ê°€
        train_score = self.model.score(X_train_scaled, y_train)
        test_score = self.model.score(X_test_scaled, y_test)
        
        print(f"âœ… ëª¨ë¸ í•™ìŠµ ì™„ë£Œ")
        print(f"í›ˆë ¨ ì •í™•ë„: {train_score:.4f}")
        print(f"í…ŒìŠ¤íŠ¸ ì •í™•ë„: {test_score:.4f}")
        
        # ONNX ë³€í™˜
        try:
            initial_type = [('float_input', FloatTensorType([None, len(feature_cols)]))]
            self.onnx_model = convert_sklearn(
                self.model, 
                initial_types=initial_type,
                target_opset=11
            )
            
            print("âœ… ONNX ë³€í™˜ ì™„ë£Œ")
            
            # ONNX ëª¨ë¸ ì €ì¥
            onnx.save_model(self.onnx_model, 'model.onnx')
            print("âœ… ONNX ëª¨ë¸ ì €ì¥ ì™„ë£Œ: model.onnx")
            
        except Exception as e:
            print(f"âŒ ONNX ë³€í™˜ ì‹¤íŒ¨: {e}")
            
        return train_score, test_score
    
    def test_onnx_model(self):
        """ONNX ëª¨ë¸ í…ŒìŠ¤íŠ¸"""
        if self.onnx_model is None:
            print("âŒ ONNX ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        # ONNX Runtime ì„¸ì…˜ ìƒì„±
        ort_session = ort.InferenceSession(
            self.onnx_model.SerializeToString(),
            providers=['CPUExecutionProvider']
        )
        
        # í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¤€ë¹„
        feature_cols = [col for col in self.data.columns if col not in ['Target', 'Species']]
        X_sample = self.data[feature_cols].iloc[:5].values
        X_sample_scaled = self.scaler.transform(X_sample).astype(np.float32)
        
        # ONNX ëª¨ë¸ë¡œ ì˜ˆì¸¡
        onnx_pred = ort_session.run(
            None, 
            {'float_input': X_sample_scaled}
        )[1]  # í™•ë¥ ê°’ ë°˜í™˜
        
        # ì›ë³¸ ëª¨ë¸ë¡œ ì˜ˆì¸¡
        sklearn_pred = self.model.predict_proba(X_sample_scaled)
        
        print("âœ… ONNX ëª¨ë¸ í…ŒìŠ¤íŠ¸ ê²°ê³¼:")
        print("ONNX ì˜ˆì¸¡ í™•ë¥  (ì²˜ìŒ 5ê°œ ìƒ˜í”Œ):")
        for i, prob in enumerate(onnx_pred):
            print(f"ìƒ˜í”Œ {i+1}: {prob}")
        
        print("\nScikit-learn ì˜ˆì¸¡ í™•ë¥ :")
        for i, prob in enumerate(sklearn_pred):
            print(f"ìƒ˜í”Œ {i+1}: {prob}")
        
        # ì°¨ì´ ê³„ì‚°
        diff = np.abs(onnx_pred - sklearn_pred)
        print(f"\ní‰ê·  ì°¨ì´: {np.mean(diff):.6f}")
        print("âœ… ëª¨ë¸ ì¼ì¹˜ì„± ê²€ì¦ ì™„ë£Œ")
        
        return onnx_pred, sklearn_pred

# ì‹¤í–‰ ì˜ˆì œ
def main():
    print("ğŸš€ Pythonê³¼ ONYX ë°ì´í„° ë¶„ì„ ì‹œì‘!")
    print("=" * 50)
    
    # ë¶„ì„ê¸° ì´ˆê¸°í™”
    analyzer = DataAnalyzer()
    
    # ë°ì´í„° ë¡œë“œ (Iris ë°ì´í„°ì…‹ ì‚¬ìš©)
    analyzer.load_iris_data()
    
    # ë°ì´í„° íƒìƒ‰
    stats = analyzer.explore_data()
    
    # ì‹œê°í™”
    print("\nğŸ“Š ë°ì´í„° ì‹œê°í™” ìƒì„± ì¤‘...")
    analyzer.create_visualizations()
    
    # ëª¨ë¸ í•™ìŠµ ë° ONNX ë³€í™˜
    print("\nğŸ¤– ëª¨ë¸ í•™ìŠµ ë° ONNX ë³€í™˜...")
    train_acc, test_acc = analyzer.train_model_and_convert_to_onnx()
    
    # ONNX ëª¨ë¸ í…ŒìŠ¤íŠ¸
    print("\nğŸ” ONNX ëª¨ë¸ í…ŒìŠ¤íŠ¸...")
    analyzer.test_onnx_model()
    
    print("\n" + "=" * 50)
    print("âœ… ëª¨ë“  ë¶„ì„ ì™„ë£Œ!")

if __name__ == "__main__":
    main()

# ì¶”ê°€ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
def create_advanced_analysis():
    """ê³ ê¸‰ ë¶„ì„ ì˜ˆì œ"""
    print("\nğŸ”¬ ê³ ê¸‰ ë¶„ì„ ì˜ˆì œ")
    
    # ì‹œê³„ì—´ ë°ì´í„° ìƒì„±
    dates = pd.date_range('2024-01-01', periods=365)
    trend = np.linspace(100, 200, 365)
    seasonal = 10 * np.sin(2 * np.pi * np.arange(365) / 365)
    noise = np.random.normal(0, 5, 365)
    values = trend + seasonal + noise
    
    ts_data = pd.DataFrame({
        'date': dates,
        'value': values
    })
    
    # ì‹œê³„ì—´ ì‹œê°í™”
    plt.figure(figsize=(12, 6))
    plt.plot(ts_data['date'], ts_data['value'], label='Time Series Data')
    plt.plot(ts_data['date'], trend, label='Trend', linestyle='--')
    plt.title('Time Series Analysis Example')
    plt.xlabel('Date')
    plt.ylabel('Value')
    plt.legend()
    plt.xticks(rotation=45)
    plt.tight_layout()
    plt.show()
    
    return ts_data

def performance_comparison():
    """ì„±ëŠ¥ ë¹„êµ ì‹œê°í™”"""
    models = ['Random Forest', 'SVM', 'Neural Network', 'ONNX Model']
    accuracy = [0.95, 0.92, 0.96, 0.95]
    inference_time = [10, 25, 15, 5]  # milliseconds
    
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))
    
    # ì •í™•ë„ ë¹„êµ
    ax1.bar(models, accuracy, color=['skyblue', 'lightgreen', 'lightcoral', 'gold'])
    ax1.set_title('Model Accuracy Comparison')
    ax1.set_ylabel('Accuracy')
    ax1.set_ylim(0.9, 1.0)
    
    # ì¶”ë¡  ì‹œê°„ ë¹„êµ
    ax2.bar(models, inference_time, color=['skyblue', 'lightgreen', 'lightcoral', 'gold'])
    ax2.set_title('Inference Time Comparison')
    ax2.set_ylabel('Time (ms)')
    
    plt.tight_layout()
    plt.show()

# ë³€ìˆ˜ ì„¤ì • ì˜ˆì œ
CONFIG = {
    'model_params': {
        'n_estimators': 100,
        'max_depth': 10,
        'random_state': 42
    },
    'data_params': {
        'test_size': 0.2,
        'n_samples': 1000,
        'n_features': 4
    },
    'visualization': {
        'figure_size': (15, 12),
        'color_palette': 'viridis',
        'style': 'seaborn-v0_8'
    }
}

print("ğŸ“ ì„¤ì • ë³€ìˆ˜ë“¤:")
for section, params in CONFIG.items():
    print(f"\n{section}:")
    for key, value in params.items():
        print(f"  - {key}: {value}")
